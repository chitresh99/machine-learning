{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Id0VlXu8rLVl"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_dataset(dataframes, oversample=False):\n",
        "    x = dataframes[dataframes.columns[:-1]].values\n",
        "    y = dataframes[dataframes.columns[-1]].values\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(x)\n",
        "\n",
        "    data = np.hstack((X, np.reshape(y, (-1, 1))))  # stacking all the arrays together like take the first array and take the second array and stack them together\n",
        "\n",
        "    if oversample:\n",
        "        ros = RandomOverSampler()\n",
        "        X, y = ros.fit_resample(X, y)  # this will take whatever is present in less quantity and make more of it i mean it will increase the data\n",
        "\n",
        "    return data, X, y\n",
        "    # x is two dimensional\n",
        "    # y is one dimensional\n",
        "    # we made our y a 2d array"
      ],
      "metadata": {
        "id": "4-Nrq9xwr4GE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('magic04.data')"
      ],
      "metadata": {
        "id": "jRWgRNe0r7ca"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, X, y = scale_dataset(df, oversample=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "TE4dO4Sur-fx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_model = GaussianNB()\n",
        "nb_model = nb_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "R2aapPR5sA6Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = nb_model.predict(X_test)"
      ],
      "metadata": {
        "id": "4G91u3jAsaXA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGev68-sssVm",
        "outputId": "4114cca0-c77c-4bf4-b1c9-7f133e83d536"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           g       0.61      0.90      0.73      2498\n",
            "           h       0.80      0.41      0.54      2435\n",
            "\n",
            "    accuracy                           0.66      4933\n",
            "   macro avg       0.71      0.66      0.64      4933\n",
            "weighted avg       0.70      0.66      0.64      4933\n",
            "\n"
          ]
        }
      ]
    }
  ]
}